# Transformers & LLM Fundamentals

Educational repository for learning Transformers and Large Language Models.

## Repository Contents

### Notebooks

- **`Introduction_to_NLP.ipynb`** - Comprehensive step-by-step tutorial covering Basics of NLP , Text processing , Vectorization techniques , Embeddings.
  
- **`transformers_demo_v0.ipynb`** - Comprehensive step-by-step tutorial covering Transformers and LLM fundamentals from raw text to AI-generated content. Includes 14 sequential sections on tokenization, embeddings, model inference, text generation, and practical applications.

- **`Transformers_demo_v1.ipynb`** - Task-based tutorial covering NLP concepts using the transformers library, including tokenization strategies, embeddings, model inference, text generation, and semantic similarity.

### Presentation

- **`Introduction_to_NLP.pptx`** - PowerPoint presentation containing theoretical foundation and information about NLP. Covers architecture, concepts, and key principles.

- **`Transformers_LLM_Theory.pptx`** - PowerPoint presentation containing theoretical foundation and information about Transformers and LLMs. Covers architecture, concepts, and key principles.

## Getting Started

1. Install dependencies:
```bash
pip install transformers torch datasets numpy pandas matplotlib seaborn scikit-learn sentence-transformers
```

2. Open notebooks in Jupyter:
```bash
jupyter notebook
```

3. Review the PPT for theoretical background before or alongside the notebooks.

## Requirements

- Python 3.8+
- Jupyter Notebook
- See installation command above for required packages

## Models Used

- GPT-2 (text generation)
- DistilBERT (embeddings)
- BERT (tokenization examples)

All models are publicly available from Hugging Face (no authentication required).
